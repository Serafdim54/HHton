import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import re
import feedparser
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from fake_useragent import UserAgent
import json
from bs4 import XMLParsedAsHTMLWarning
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø–∞—Ä—Å–∏–Ω–≥–µ XML –∫–∞–∫ HTML
warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

# URL –¥–ª—è –†–ò–ê –ù–æ–≤–æ—Å—Ç–µ–π
URL_POLITICS = "https://ria.ru/politics/"
URL_SCIENCE = "https://ria.ru/science/"
URL_HEALTH = "https://ria.ru/health/"

class AdvancedNewsParser:
    """
    –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.setup_session()
        self.selenium_driver = None
        
    def setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–µ—Å—Å–∏–∏ —Å —Ä–∞–Ω–¥–æ–º–Ω—ã–º–∏ User-Agent"""
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def get_selenium_driver(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Selenium –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if not self.selenium_driver:
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument(f"user-agent={self.ua.random}")
            chrome_options.add_argument("--window-size=1920,1080")
            
            self.selenium_driver = webdriver.Chrome(options=chrome_options)
            self.selenium_driver.set_page_load_timeout(30)
        
        return self.selenium_driver
    
    def close_selenium(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ Selenium –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.selenium_driver:
            self.selenium_driver.quit()
            self.selenium_driver = None
    
    def _make_request(self, url: str, use_selenium: bool = False) -> Optional[BeautifulSoup]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Selenium –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        """
        try:
            if use_selenium:
                print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è: {url}")
                driver = self.get_selenium_driver()
                driver.get(url)
                
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                time.sleep(3)
                html = driver.page_source
                return BeautifulSoup(html, 'html.parser')
            else:
                print(f"üåê –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {url}")
                time.sleep(random.uniform(1, 3))
                self.session.headers['User-Agent'] = self.ua.random
                
                response = self.session.get(url, timeout=15)
                response.encoding = 'utf-8'
                print(f"‚úÖ –°—Ç–∞—Ç—É—Å: {response.status_code}")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, XML —ç—Ç–æ –∏–ª–∏ HTML
                if any(xml_indicator in url.lower() for xml_indicator in ['rss', 'xml', 'feed', 'export']):
                    print("üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º XML –ø–∞—Ä—Å–µ—Ä –¥–ª—è RSS")
                    return BeautifulSoup(response.content, 'xml')
                else:
                    return BeautifulSoup(response.text, 'html.parser')
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
            return None

    def parse_with_fallback_strategy(self, url: str, source_type: str) -> List[Dict]:
        """
        –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è –†–ò–ê –ù–æ–≤–æ—Å—Ç–µ–π
        """
        print(f"üéØ –ó–∞–ø—É—Å–∫–∞–µ–º –∫–∞—Å–∫–∞–¥–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è: {url}")
        
        # –î–ª—è –†–ò–ê –ù–æ–≤–æ—Å—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
        if 'ria.ru' in url and not ('rss' in url or 'export' in url):
            print("üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –†–ò–ê –ù–æ–≤–æ—Å—Ç–µ–π")
            return self._parse_ria_news_advanced(url, source_type)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –ü–∞—Ä—Å–∏–Ω–≥ RSS
        if any(rss_indicator in url.lower() for rss_indicator in ['rss', 'export', 'feed']):
            news = self._parse_rss_feed_advanced(url)
            if news:
                print(f"‚úÖ RSS —É—Å–ø–µ—à–Ω–æ: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                return news
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π HTML –ø–∞—Ä—Å–∏–Ω–≥
        soup_static = self._make_request(url, use_selenium=False)
        if soup_static:
            news = self._extract_news_advanced(soup_static, url, source_type)
            if news:
                print(f"‚úÖ –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                return news
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ Selenium
        print("üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É...")
        soup_dynamic = self._make_request(url, use_selenium=True)
        if soup_dynamic:
            news = self._extract_news_advanced(soup_dynamic, url, source_type)
            if news:
                print(f"‚úÖ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                return news
        
        print("‚ùå –í—Å–µ –º–µ—Ç–æ–¥—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return []

    # ===== –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø –†–ò–ê –ù–û–í–û–°–¢–ï–ô =====
    
    def _parse_ria_news_advanced(self, url: str, category: str) -> List[Dict]:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –†–ò–ê –ù–æ–≤–æ—Å—Ç–µ–π"""
        print(f"üîç –ü–∞—Ä—Å–∏–º –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏: {url}")
        soup = self._make_request(url, use_selenium=False)
        if not soup:
            return []
        
        news_items = []
        seen_links = set()
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –†–ò–ê
        item_selectors = ['.cell-list__item', '.list-item', '.news-item', '[data-type="news"]']
        items = []
        
        for selector in item_selectors:
            found_items = soup.select(selector)
            if found_items:
                items.extend(found_items)
        
        for item in items:
            try:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                title = self._extract_ria_title(item)
                if not title:
                    continue
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–∫–∏
                link = self._extract_ria_link(item)
                if not link or link in seen_links:
                    continue
                seen_links.add(link)
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏
                date, time = self._parse_ria_date_time(item)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image = self._extract_ria_image_url(item)
                
                news_item = {
                    'title': title,
                    'date': date,
                    'time': time,
                    'image': image,
                    'link': link,
                    'source': 'RIA.ru'
                }
                
                news_items.append(news_item)
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ –†–ò–ê: {e}")
                continue
        
        print(f"‚úÖ –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏: —Å–æ–±—Ä–∞–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return news_items
    
    def _extract_ria_title(self, item) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –†–ò–ê"""
        title_selectors = [
            '.cell-list__item-title', 
            '.list-item__title', 
            'h2', 
            'h3',
            '.news-item__title'
        ]
        
        title_tag = None
        for selector in title_selectors:
            title_tag = item.select_one(selector)
            if title_tag:
                break
        
        if not title_tag:
            title_tag = item
        
        title = title_tag.get_text().strip()
        return title if title and len(title) >= 10 else None
    
    def _extract_ria_link(self, item) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –†–ò–ê"""
        link = item.get('href', '')
        
        if not link:
            link_tag = item.select_one('a[href]')
            if link_tag:
                link = link_tag.get('href', '')
        
        if not link:
            return None
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –¥–ª—è –†–ò–ê
        if link.startswith('/'):
            link = 'https://ria.ru' + link
        elif link.startswith('//'):
            link = 'https:' + link
        
        return link
    
    def _parse_ria_date_time(self, item) -> tuple[str, str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –†–ò–ê"""
        date, time = '', ''
        
        date_selectors = ['.cell-info__date', '[data-type="date"]', '.list-item__info']
        date_element = None
        
        for selector in date_selectors:
            date_element = item.select_one(selector)
            if date_element:
                break
        
        if not date_element:
            return self.get_today_date(), ''
        
        date_text = date_element.get_text().strip()
        
        if ',' in date_text:
            parts = date_text.split(',', 1)
            if len(parts) == 2:
                date_part, time_part = parts
                date = date_part.strip()
                time = self._extract_time_from_text(time_part.strip())
        else:
            if ':' in date_text:
                time = self._extract_time_from_text(date_text)
                date = '–°–µ–≥–æ–¥–Ω—è' if time else ''
            else:
                date = date_text
        
        if time and not date:
            date = '–°–µ–≥–æ–¥–Ω—è'
            
        return date or self.get_today_date(), time
    
    def _extract_ria_image_url(self, item) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –†–ò–ê"""
        image = ''
        
        img_container = item.select_one('.cell-list__item-img')
        if img_container:
            img_tag = img_container.select_one('img')
            if img_tag:
                image = img_tag.get('src') or img_tag.get('data-src') or ''
        
        if not image:
            img_tag = item.select_one('img')
            if img_tag:
                image = img_tag.get('src') or img_tag.get('data-src') or ''
        
        if not image:
            div_with_bg = item.select_one('[style*="background-image"]')
            if div_with_bg and div_with_bg.get('style'):
                style = div_with_bg['style']
                if 'url(' in style:
                    start = style.find('url(')
                    end = style.find(')', start)
                    if start != -1 and end != -1:
                        image = style[start+4:end].strip('"\'')
        
        if image:
            if image.startswith('//'):
                image = 'https:' + image
            elif image.startswith('/'):
                image = 'https://ria.ru' + image
        
        return image
    
    def _extract_time_from_text(self, time_text: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Ä–µ–º—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ß–ß:–ú–ú"""
        time_clean = ''
        colon_found = False
        digits_after_colon = 0
        
        for char in time_text:
            if char == ':':
                colon_found = True
                time_clean += char
            elif char.isdigit():
                if colon_found:
                    digits_after_colon += 1
                    if digits_after_colon <= 2:
                        time_clean += char
                    else:
                        break
                else:
                    time_clean += char
            else:
                break
        
        return time_clean

    # ===== –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† –î–õ–Ø –î–†–£–ì–ò–• –ò–°–¢–û–ß–ù–ò–ö–û–í =====
    
    def _parse_rss_feed_advanced(self, rss_url: str) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        print(f"üì° –ü–∞—Ä—Å–∏–º RSS: {rss_url}")
        news_items = []
        
        try:
            feed = feedparser.parse(rss_url)
            
            if not feed.entries:
                print("‚ö†Ô∏è RSS feed –ø—É—Å—Ç –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return []
            
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ RSS –∑–∞–ø–∏—Å–µ–π: {len(feed.entries)}")
            
            for i, entry in enumerate(feed.entries[:20]):
                try:
                    pub_date = self._parse_rss_date(entry)
                    image_url = self._extract_rss_image(entry)
                    
                    news_item = {
                        'title': entry.title,
                        'date': pub_date,
                        'time': self._extract_time_from_rss(entry),
                        'image': image_url,
                        'link': entry.link,
                        'source': self._extract_source_name(rss_url),
                        'description': getattr(entry, 'description', '')[:200] + '...' if hasattr(entry, 'description') else ''
                    }
                    news_items.append(news_item)
                    
                    if i < 3:
                        print(f"   ‚úÖ {i+1}. {entry.title[:60]}...")
                        
                except Exception as e:
                    print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
                    continue
                    
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ RSS –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        
        return news_items
    
    def _parse_rss_date(self, entry) -> str:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ RSS –≤ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                dt = datetime(*entry.published_parsed[:6])
                return dt.strftime("%d.%m.%Y")
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                dt = datetime(*entry.updated_parsed[:6])
                return dt.strftime("%d.%m.%Y")
        except:
            pass
        
        return self.get_today_date()
    
    def _extract_rss_image(self, entry) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ RSS –∑–∞–ø–∏—Å–∏"""
        image_url = ''
        
        if hasattr(entry, 'links'):
            for link in entry.links:
                if link.get('type', '').startswith('image/'):
                    image_url = link.href
                    break
                elif link.get('rel', '') == 'enclosure':
                    image_url = link.href
                    break
        
        if not image_url and hasattr(entry, 'content'):
            for content in entry.content:
                if content.get('type', '').startswith('image/'):
                    image_url = content.get('url', '')
                    break
        
        if not image_url and hasattr(entry, 'media_thumbnail'):
            image_url = entry.media_thumbnail[0]['url']
        
        return image_url
    
    def _extract_news_advanced(self, soup: BeautifulSoup, url: str, source_type: str) -> List[Dict]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML –¥–ª—è –¢–ê–°–°, –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å –∏ –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä
        """
        news_items = []
        source_name = self._extract_source_name(url)
        
        print(f"üîç –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è {source_name}")
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        extraction_methods = {
            'tass.ru': self._extract_tass_news,
            'interfax.ru': self._extract_interfax_news,
            'doctorpiter.ru': self._extract_doctorpiter_news
        }
        
        method = extraction_methods.get(source_name, self._extract_generic_news)
        news_items = method(soup, url)
        
        return news_items
    
    def _extract_tass_news(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """–°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è TASS"""
        news_items = []
        
        selectors = [
            '.news-line__item',
            '.news-list__item', 
            '.content-big-newslist__item',
            '.b-material-list__item',
            'article'
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                print(f"‚úÖ TASS: –Ω–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}': {len(articles)}")
                
                for article in articles[:20]:
                    try:
                        title_elem = article.select_one('.news-line__title, .news-list__title, .b-material-list__title, h2, h3, h4, a')
                        if not title_elem:
                            continue
                            
                        title = title_elem.get_text().strip()
                        if len(title) < 5:
                            continue
                        
                        link_elem = article.select_one('a[href]')
                        if not link_elem:
                            continue
                            
                        link = self._normalize_url(link_elem.get('href'), 'tass.ru')
                        if not link:
                            continue
                        
                        image_url = self._extract_tass_image(article)
                        
                        news_item = {
                            'title': title[:100] + "..." if len(title) > 100 else title,
                            'date': self.get_today_date(),
                            'time': '',
                            'image': image_url,
                            'link': link,
                            'source': 'TASS'
                        }
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ TASS: {e}")
                        continue
                
                if news_items:
                    break
        
        return news_items
    
    def _extract_interfax_news(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å - –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ —Å —Ö–æ—Ä–æ—à–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        news_items = []
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å
        selectors = [
            '.newsPage__list .timeline__item',
            '.newsList .newsItem',
            '.main-newslist .news-item',
            '.news-feed-list .news-feed-item',
            'article',
            '.news'
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                print(f"‚úÖ –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å: –Ω–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}': {len(articles)}")
                
                for article in articles[:20]:
                    try:
                        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å
                        title_elem = article.select_one(
                            '.timeline__item-title, .newsItem__title, '
                            '.news-item__title, h3, h4, .title, a'
                        )
                        if not title_elem:
                            continue
                            
                        title = title_elem.get_text().strip()
                        if len(title) < 10 or len(title) > 300:
                            continue
                        
                        link_elem = article.select_one('a[href]')
                        if not link_elem:
                            continue
                            
                        link = self._normalize_url(link_elem.get('href'), 'interfax.ru')
                        if not link or 'interfax.ru' not in link:
                            continue
                        
                        image_url = self._extract_interfax_image(article)
                        
                        news_item = {
                            'title': title[:100] + "..." if len(title) > 100 else title,
                            'date': self.get_today_date(),
                            'time': '',
                            'image': image_url,
                            'link': link,
                            'source': '–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å'
                        }
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å: {e}")
                        continue
                
                if len(news_items) >= 5:
                    break
        
        return news_items
    
    def _extract_doctorpiter_news(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä - –Ω–∞–¥–µ–∂–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –ø–æ—Ä—Ç–∞–ª"""
        news_items = []
        
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä
        selectors = [
            '.news-item',
            '.article-preview',
            '.news-list-item',
            '.item-news',
            'article.news',
            '.b-news-item'
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                print(f"‚úÖ –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä: –Ω–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}': {len(articles)}")
                
                for article in articles[:20]:
                    try:
                        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä
                        title_elem = article.select_one(
                            '.news-item__title, .article-preview__title, '
                            '.news-list-item__title, .item-news__title, '
                            'h2, h3, h4, .title, a'
                        )
                        if not title_elem:
                            continue
                            
                        title = title_elem.get_text().strip()
                        if len(title) < 10 or len(title) > 300:
                            continue
                        
                        link_elem = article.select_one('a[href]')
                        if not link_elem:
                            continue
                            
                        link = self._normalize_url(link_elem.get('href'), 'doctorpiter.ru')
                        if not link or 'doctorpiter.ru' not in link:
                            continue
                        
                        image_url = self._extract_doctorpiter_image(article)
                        
                        news_item = {
                            'title': title[:100] + "..." if len(title) > 100 else title,
                            'date': self.get_today_date(),
                            'time': '',
                            'image': image_url,
                            'link': link,
                            'source': '–î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä'
                        }
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä: {e}")
                        continue
                
                if len(news_items) >= 5:
                    break
        
        return news_items
    
    def _extract_interfax_image(self, article) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å"""
        img_selectors = [
            'img',
            '.timeline__item-img img',
            '.newsItem__image img',
            '.news-item__image img',
            '[data-src]'
        ]
        
        for selector in img_selectors:
            img_elem = article.select_one(selector)
            if img_elem:
                src = img_elem.get('src') or img_elem.get('data-src')
                if src:
                    return self._normalize_url(src, 'interfax.ru')
        return ''
    
    def _extract_doctorpiter_image(self, article) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä"""
        img_selectors = [
            'img',
            '.news-item__image img',
            '.article-preview__image img',
            '.news-list-item__image img',
            '.item-news__image img',
            '[data-src]'
        ]
        
        for selector in img_selectors:
            img_elem = article.select_one(selector)
            if img_elem:
                src = img_elem.get('src') or img_elem.get('data-src')
                if src:
                    return self._normalize_url(src, 'doctorpiter.ru')
        return ''
    
    def _extract_tass_image(self, article) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è TASS"""
        img_selectors = [
            'img',
            '.news-line__image img',
            '.b-material-list__image img',
            '[data-src]'
        ]
        
        for selector in img_selectors:
            img_elem = article.select_one(selector)
            if img_elem:
                src = img_elem.get('src') or img_elem.get('data-src')
                if src:
                    return self._normalize_url(src, 'tass.ru')
        return ''
    
    def _extract_generic_news(self, soup: BeautifulSoup, url: str) -> List[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        news_items = []
        
        universal_selectors = [
            'article',
            '.news-item',
            '.item',
            '.card',
            '.post',
            '[class*="news"]',
            '[class*="article"]',
            '.news'
        ]
        
        for selector in universal_selectors:
            articles = soup.select(selector)
            if articles:
                print(f"üåê –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É '{selector}'")
                
                for article in articles[:15]:
                    try:
                        title_elem = article.select_one('h1, h2, h3, h4, h5, .title, .heading, [class*="title"], [class*="heading"]')
                        if not title_elem:
                            continue
                            
                        title = title_elem.get_text().strip()
                        if len(title) < 5 or len(title) > 500:
                            continue
                        
                        link_elem = article.select_one('a[href]')
                        if not link_elem:
                            continue
                            
                        link = link_elem.get('href')
                        if not link or link.startswith('javascript:'):
                            continue
                            
                        if link.startswith('/'):
                            base_domain = re.findall(r'https?://[^/]+', url)[0]
                            link = base_domain + link
                        elif not link.startswith('http'):
                            link = url + link
                        
                        img_elem = article.select_one('img')
                        image_url = img_elem.get('src') if img_elem else ''
                        
                        news_item = {
                            'title': title[:100] + "..." if len(title) > 100 else title,
                            'date': self.get_today_date(),
                            'time': '',
                            'image': image_url,
                            'link': link,
                            'source': self._extract_source_name(url)
                        }
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        continue
                
                if news_items:
                    break
        
        return news_items
    
    def _normalize_url(self, url: str, source_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if not url or url.startswith('javascript:'):
            return ''
        
        url = url.strip()
        
        source_domains = {
            'ria.ru': 'https://ria.ru',
            'tass.ru': 'https://tass.ru', 
            'interfax.ru': 'https://www.interfax.ru',
            'doctorpiter.ru': 'https://doctorpiter.ru'
        }
        
        base_domain = source_domains.get(source_name, '')
        
        if url.startswith('//'):
            return 'https:' + url
        elif url.startswith('/'):
            return base_domain + url
        elif not url.startswith('http'):
            return base_domain + '/' + url
        
        return url
    
    def _extract_time_from_rss(self, entry) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ RSS"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                dt = datetime(*entry.published_parsed[:6])
                return dt.strftime("%H:%M")
        except:
            pass
        return ''
    
    def _extract_source_name(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ URL"""
        if 'ria.ru' in url:
            return 'RIA.ru'
        elif 'tass.ru' in url:
            return 'TASS'
        elif 'interfax.ru' in url:
            return '–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å'
        elif 'doctorpiter.ru' in url:
            return '–î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä'
        else:
            domain = re.findall(r'https?://([^/]+)', url)
            return domain[0] if domain else 'Unknown'
    
    def get_today_date(self) -> str:
        return datetime.now().strftime("%d.%m.%Y")
    
    # ===== –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ü–û–õ–£–ß–ï–ù–ò–Ø –ü–û–õ–ù–û–ì–û –¢–ï–ö–°–¢–ê –ò –ü–†–ï–í–¨–Æ =====
    
    def _is_table_of_contents(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ–º"""
        toc_indicators = [
            '–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '—Å–æ–¥–µ—Ä–∂–∏—Ç', '–≤ —Å—Ç–∞—Ç—å–µ', 
            '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ', 'table of contents', 'toc',
            '–≤–≤–µ–¥–µ–Ω–∏–µ', '–∑–∞–≥–æ–ª–æ–≤–æ–∫', '—Ä–∞–∑–¥–µ–ª', '—á–∞—Å—Ç—å', '–≥–ª–∞–≤–∞'
        ]
        
        text_lower = text.lower().strip()
        if len(text_lower) < 50:
            return False
            
        for indicator in toc_indicators:
            if indicator in text_lower:
                return True
        
        toc_patterns = [
            r'^\d+\.\s',
            r'^[ivx]+\.\s',
            r'^—Ä–∞–∑–¥–µ–ª\s+\d+',
            r'^—á–∞—Å—Ç—å\s+\d+',
            r'^–≥–ª–∞–≤–∞\s+\d+',
        ]
        
        first_line = text_lower.split('\n')[0] if '\n' in text_lower else text_lower
        for pattern in toc_patterns:
            if re.search(pattern, first_line):
                return True
        
        return False
    
    def _extract_news_preview(self, text: str, preview_length: int = 300) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ–≤—å—é –Ω–æ–≤–æ—Å—Ç–∏, –ø—Ä–æ–ø—É—Å–∫–∞—è –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ"""
        lines = text.split('\n')
        news_lines = []
        
        skip_toc = True
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if skip_toc:
                if self._is_table_of_contents(line):
                    continue
                if len(line) > 50 and not self._is_table_of_contents(line):
                    skip_toc = False
                    news_lines.append(line)
            else:
                news_lines.append(line)
        
        if not news_lines:
            news_lines = [line.strip() for line in lines if line.strip()]
        
        preview_text = ' '.join(news_lines)
        
        if len(preview_text) > preview_length:
            preview_text = preview_text[:preview_length]
            last_space = preview_text.rfind(' ')
            if last_space > preview_length * 0.7:
                preview_text = preview_text[:last_space]
            preview_text += '...'
        
        return preview_text
    
    def get_full_article_text(self, url: str, preserve_formatting: bool = True) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print(f"üìñ –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {url}")
        
        # –î–ª—è –†–ò–ê –ù–æ–≤–æ—Å—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥
        if 'ria.ru' in url:
            return self._get_ria_full_article_text(url, preserve_formatting)
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –º–µ—Ç–æ–¥
        soup = self._make_request(url, use_selenium=False)
        if not soup:
            soup = self._make_request(url, use_selenium=True)
        
        if not soup:
            return ''
        
        content_selectors = [
            'div.article__body',
            'div.article-text',
            'div.b-text',
            'article',
            'div.content',
            'div.post-content',
            '[class*="article"]',
            '[class*="content"]'
        ]
        
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                unwanted_selectors = [
                    'script', 'style', '.ad', '.banner', '.social', '.share',
                    '.article__info', '.article__meta', '.article__tags',
                    '.recommended', '.related', '.comments', '.advertisement'
                ]
                
                for unwanted in unwanted_selectors:
                    for elem in content_div.select(unwanted):
                        elem.decompose()
                
                if preserve_formatting:
                    return self._extract_formatted_text(content_div)
                else:
                    text = content_div.get_text().strip()
                    if len(text) > 200:
                        return self._clean_text(text)
        
        return ''
    
    def _get_ria_full_article_text(self, url: str, preserve_formatting: bool = True) -> str:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –†–ò–ê"""
        soup = self._make_request(url)
        if not soup:
            return ''
        
        content_selectors = [
            'div.article__body',
            'div.article__text',
            'article',
            '.content',
            '.post-content',
            '[class*="article"]',
            '[class*="content"]'
        ]
        
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                unwanted_selectors = [
                    'script', 'style', '.ad', '.banner', '.social', '.share',
                    '.article__info', '.article__meta', '.article__tags',
                    '.recommended', '.related', '.comments', '.advertisement'
                ]
                
                for unwanted in unwanted_selectors:
                    for elem in content_div.select(unwanted):
                        elem.decompose()
                
                if preserve_formatting:
                    return self._extract_formatted_text(content_div)
                else:
                    text = content_div.get_text().strip()
                    if len(text) > 100:
                        return text
        
        return ''
    
    def get_article_preview(self, url: str, preview_length: int = 300) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–≤—å—é —Å—Ç–∞—Ç—å–∏ –±–µ–∑ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è"""
        full_text = self.get_full_article_text(url, preserve_formatting=True)
        if not full_text:
            return ''
        
        return self._extract_news_preview(full_text, preview_length)
    
    def _extract_formatted_text(self, content_div) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫"""
        temp_div = BeautifulSoup(content_div.prettify(), 'html.parser')
        
        for tag in temp_div.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            tag.insert_after('\n\n')
            tag.insert_before('\n\n')
        
        for tag in temp_div.find_all('p'):
            tag.insert_after('\n\n')
        
        for tag in temp_div.find_all(['ul', 'ol']):
            tag.insert_after('\n')
            for li in tag.find_all('li'):
                li.insert_after('\n')
        
        for tag in temp_div.find_all('div'):
            if tag.get_text(strip=True):
                child_tags = tag.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol'])
                if not child_tags:
                    tag.insert_after('\n')
        
        text = temp_div.get_text()
        
        lines = []
        for line in text.splitlines():
            line = line.strip()
            if line:
                lines.append(line)
        
        formatted_text = '\n'.join(lines)
        formatted_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', formatted_text)
        
        return formatted_text.strip()
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return '\n'.join(lines)
    
    # ===== –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –ü–ê–†–°–ò–ù–ì–ê –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú =====
    
    def parse_category_news(self, category: str) -> Dict[str, List]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        """
        print(f"\n{'='*60}")
        print(f"üöÄ –ó–ê–ü–£–°–ö –ü–ê–†–°–ò–ù–ì–ê: {category.upper()}")
        print(f"{'='*60}")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources_config = {
            'politics': [
                # –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
                ('https://ria.ru/politics/', 'HTML'),
                
                # –¢–ê–°–° (RSS + HTML) - –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏
                ('https://tass.ru/rss/v2.xml', 'RSS'),
                ('https://tass.ru/politika', 'HTML'),
                
                # –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å (RSS + HTML) - –æ—Ç–ª–∏—á–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏
                ('https://www.interfax.ru/rss.asp', 'RSS'),
                ('https://www.interfax.ru/politics/', 'HTML')
                
                # Lenta.ru —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –ø–æ–ª–∏—Ç–∏–∫–∏ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º
            ],
            'science': [
                # –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏
                ('https://ria.ru/science/', 'HTML'),
                
                # –¢–ê–°–° –¥–ª—è –Ω–∞—É–∫–∏
                ('https://tass.ru/rss/v2.xml', 'RSS'),
                ('https://tass.ru/nauka', 'HTML'),
                
                # –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å –¥–ª—è –Ω–∞—É–∫–∏
                ('https://www.interfax.ru/rss.asp', 'RSS'),
                ('https://www.interfax.ru/science/', 'HTML')
            ],
            'health': [
                # –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏
                ('https://ria.ru/health/', 'HTML'),
                
                # –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä (RSS + HTML) - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫
                ('https://doctorpiter.ru/rss/', 'RSS'),
                ('https://doctorpiter.ru/news/', 'HTML'),
                
                # –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å –¥–ª—è –∑–¥–æ—Ä–æ–≤—å—è
                ('https://www.interfax.ru/rss.asp', 'RSS'),
                ('https://www.interfax.ru/health/', 'HTML')
                
                # Lenta.ru –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ –î–æ–∫—Ç–æ—Ä –ü–∏—Ç–µ—Ä
            ]
        }
        
        if category not in sources_config:
            return {'news': [], 'statistics': {'error': 'Unknown category'}}
        
        all_news = []
        source_stats = {}
        successful_sources = []
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
        for url, parser_type in sources_config[category]:
            source_name = self._extract_source_name(url)
            source_key = f"{source_name}_{parser_type}"
            
            print(f"\nüîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {source_key}")
            print(f"   URL: {url}")
            
            try:
                news_from_source = self.parse_with_fallback_strategy(url, category)
                count = len(news_from_source)
                source_stats[source_key] = count
                
                if count > 0:
                    successful_sources.append(source_key)
                    all_news.extend(news_from_source)
                    print(f"‚úÖ –£–°–ü–ï–•: {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
                else:
                    print(f"‚ùå –ù–û–í–û–°–¢–ï–ô –ù–ï –ù–ê–ô–î–ï–ù–û")
                
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"üí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
                source_stats[source_key] = 0
        
        self.close_selenium()
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_news = []
        seen_titles = set()
        seen_links = set()
        
        for news in all_news:
            title_key = news['title'].strip().lower()[:50]
            link_key = news['link']
            
            if title_key not in seen_titles and link_key not in seen_links:
                seen_titles.add(title_key)
                seen_links.add(link_key)
                unique_news.append(news)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–ò –ö–ê–¢–ï–ì–û–†–ò–ò: {category.upper()}")
        print(f"{'='*60}")
        
        total_collected = sum(source_stats.values())
        total_unique = len(unique_news)
        
        print(f"üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {total_collected}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {total_unique}")
        print(f"   –†–∞–±–æ—Ç–∞—é—â–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(successful_sources)}/{len(sources_config[category])}")
        
        print(f"\nüìã –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú:")
        for source, count in source_stats.items():
            status = "‚úÖ" if count > 0 else "‚ùå"
            print(f"   {status} {source}: {count} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        return {
            'news': unique_news[:25],
            'statistics': {
                'total_collected': total_collected,
                'total_unique': total_unique,
                'successful_sources': len(successful_sources),
                'total_sources': len(sources_config[category]),
                'sources': source_stats
            }
        }


# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø–∞—Ä—Å–µ—Ä–∞
advanced_parser = AdvancedNewsParser()

# ===== –§–ê–ë–†–ò–ß–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –û–ë–†–ê–¢–ù–û–ô –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò =====

"""
===============================
===       POLITICS       ===
===============================
"""

def parse_latest_news_politics():
    return advanced_parser.parse_category_news('politics')

def get_full_article_text_politics(url):
    return advanced_parser.get_full_article_text(url)

def get_article_preview_politics(url, preview_length=300):
    return advanced_parser.get_article_preview(url, preview_length)


"""
===============================
===      SCIENCE       ===
===============================
"""

def parse_latest_news_science():
    return advanced_parser.parse_category_news('science')

def get_full_article_text_science(url):
    return advanced_parser.get_full_article_text(url)

def get_article_preview_science(url, preview_length=300):
    return advanced_parser.get_article_preview(url, preview_length)


"""
===============================
===       HEALTH      ===
===============================
"""

def parse_latest_news_health():
    return advanced_parser.parse_category_news('health')

def get_full_article_text_health(url):
    return advanced_parser.get_full_article_text(url)

def get_article_preview_health(url, preview_length=300):
    return advanced_parser.get_article_preview(url, preview_length)
